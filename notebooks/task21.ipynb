{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0c80615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google_play_scraper import Sort, reviews\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import time\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb895ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce309e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>bank</th>\n",
       "      <th>source</th>\n",
       "      <th>translated_review</th>\n",
       "      <th>lang_detected</th>\n",
       "      <th>sentiment_label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Why don’t your ATMs support account-to-accoun...</td>\n",
       "      <td>4</td>\n",
       "      <td>2025-06-06 09:54:11</td>\n",
       "      <td>Cbe</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>\"Why don’t your ATMs support account-to-accoun...</td>\n",
       "      <td>en</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.996465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is this app problem???</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-05 22:16:56</td>\n",
       "      <td>Cbe</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>what is this app problem???</td>\n",
       "      <td>en</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.999623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the app is proactive and a good connections.</td>\n",
       "      <td>5</td>\n",
       "      <td>2025-06-05 15:55:10</td>\n",
       "      <td>Cbe</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>the app is proactive and a good connections.</td>\n",
       "      <td>en</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I cannot send to cbebirr app. through this app.</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-06-05 11:12:49</td>\n",
       "      <td>Cbe</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>I cannot send to cbebirr app. through this app.</td>\n",
       "      <td>en</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.995335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not functional</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-06-05 07:38:12</td>\n",
       "      <td>Cbe</td>\n",
       "      <td>Google Play</td>\n",
       "      <td>not functional</td>\n",
       "      <td>en</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.999779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  rating  \\\n",
       "0  \"Why don’t your ATMs support account-to-accoun...       4   \n",
       "1                        what is this app problem???       1   \n",
       "2       the app is proactive and a good connections.       5   \n",
       "3    I cannot send to cbebirr app. through this app.       3   \n",
       "4                                     not functional       1   \n",
       "\n",
       "                  date bank       source  \\\n",
       "0  2025-06-06 09:54:11  Cbe  Google Play   \n",
       "1  2025-06-05 22:16:56  Cbe  Google Play   \n",
       "2  2025-06-05 15:55:10  Cbe  Google Play   \n",
       "3  2025-06-05 11:12:49  Cbe  Google Play   \n",
       "4  2025-06-05 07:38:12  Cbe  Google Play   \n",
       "\n",
       "                                   translated_review lang_detected  \\\n",
       "0  \"Why don’t your ATMs support account-to-accoun...            en   \n",
       "1                        what is this app problem???            en   \n",
       "2       the app is proactive and a good connections.            en   \n",
       "3    I cannot send to cbebirr app. through this app.            en   \n",
       "4                                     not functional            en   \n",
       "\n",
       "  sentiment_label  sentiment_score  \n",
       "0        NEGATIVE         0.996465  \n",
       "1        NEGATIVE         0.999623  \n",
       "2        POSITIVE         0.999868  \n",
       "3        NEGATIVE         0.995335  \n",
       "4        NEGATIVE         0.999779  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load your preprocessed review file\n",
    "df = pd.read_csv(\"../data/reviews_with_sentimentfinal.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3f61348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Perform Sentiment Analysis\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Apply it on a sample (for speed, you can scale up later)\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text[:512])[0]  # Truncate long texts\n",
    "        return result['label'], result['score']\n",
    "    except:\n",
    "        return 'ERROR', 0.0\n",
    "\n",
    "# Apply to your reviews\n",
    "df[['sentiment_label', 'sentiment_score']] = df['review'].apply(lambda x: pd.Series(analyze_sentiment(str(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e228fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        bank  rating  sentiment_score\n",
      "0   Absiniya       1         0.989370\n",
      "1   Absiniya       2         0.964561\n",
      "2   Absiniya       3         0.983056\n",
      "3   Absiniya       4         0.963430\n",
      "4   Absiniya       5         0.977867\n",
      "5        Cbe       1         0.994166\n",
      "6        Cbe       2         0.998415\n",
      "7        Cbe       3         0.986878\n",
      "8        Cbe       4         0.956228\n",
      "9        Cbe       5         0.984998\n",
      "10    Dashin       1         0.998587\n",
      "11    Dashin       2         0.986041\n",
      "12    Dashin       3         0.997215\n",
      "13    Dashin       4         0.994640\n",
      "14    Dashin       5         0.991836\n"
     ]
    }
   ],
   "source": [
    "#Step 3: Aggregate Sentiment\n",
    "# Group by bank and rating\n",
    "sentiment_summary = df.groupby(['bank', 'rating'])[['sentiment_score']].mean().reset_index()\n",
    "print(sentiment_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f41b019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Keyword & Theme Extraction (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55a7b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Summary:\n",
      "        bank  rating  sentiment_score\n",
      "0  Absiniya       1         0.989370\n",
      "1  Absiniya       2         0.964561\n",
      "2  Absiniya       3         0.983056\n",
      "3  Absiniya       4         0.963430\n",
      "4  Absiniya       5         0.977867\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m keywords_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Example: Extract top keywords for CBE\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m keywords_cbe \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbank\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCBE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_review\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop Keywords for CBE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, keywords_cbe\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# -------------------- GROUP KEYWORDS INTO THEMES --------------------\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Manual theme mapping based on observed keywords\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 34\u001b[0m, in \u001b[0;36mextract_keywords\u001b[1;34m(text_series, max_features)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_keywords\u001b[39m(text_series, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m     33\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, max_features\u001b[38;5;241m=\u001b[39mmax_features, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m---> 34\u001b[0m     tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_series\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     features \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m     36\u001b[0m     scores \u001b[38;5;241m=\u001b[39m tfidf_matrix\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mA1\n",
      "File \u001b[1;32md:\\kaimtenx\\week22\\bank-review-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32md:\\kaimtenx\\week22\\bank-review-analysis\\.venv\\lib\\site-packages\\sklearn\\base.py:1363\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1359\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1360\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1361\u001b[0m     )\n\u001b[0;32m   1362\u001b[0m ):\n\u001b[1;32m-> 1363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\kaimtenx\\week22\\bank-review-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32md:\\kaimtenx\\week22\\bank-review-analysis\\.venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# -------------------- SETUP SENTIMENT ANALYSIS --------------------\n",
    "# Load sentiment analysis pipeline using DistilBERT\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Apply sentiment model to review text (truncate if too long)\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text[:512])[0]\n",
    "        return result['label'], result['score']\n",
    "    except:\n",
    "        return 'ERROR', 0.0\n",
    "\n",
    "# Run sentiment analysis on the review column\n",
    "df[['sentiment_label', 'sentiment_score']] = df['review'].astype(str).apply(lambda x: pd.Series(analyze_sentiment(x)))\n",
    "\n",
    "# -------------------- AGGREGATE SENTIMENT BY BANK + RATING --------------------\n",
    "# Calculate mean sentiment scores grouped by bank and rating\n",
    "sentiment_summary = df.groupby(['bank', 'rating'])[['sentiment_score']].mean().reset_index()\n",
    "print(\"Sentiment Summary:\\n\", sentiment_summary.head())\n",
    "\n",
    "# -------------------- TEXT CLEANING FOR KEYWORD EXTRACTION --------------------\n",
    "# Clean and normalize text\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df['cleaned_review'] = df['review'].astype(str).apply(clean_text)\n",
    "\n",
    "# -------------------- KEYWORD EXTRACTION USING TF-IDF --------------------\n",
    "# Use TF-IDF to identify important words/phrases\n",
    "def extract_keywords(text_series, max_features=30):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features, ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_series)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.sum(axis=0).A1\n",
    "    keywords_df = pd.DataFrame({'keyword': features, 'score': scores})\n",
    "    return keywords_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# Example: Extract top keywords for CBE\n",
    "keywords_cbe = extract_keywords(df[df['bank'] == 'CBE']['cleaned_review'])\n",
    "print(\"\\nTop Keywords for CBE:\\n\", keywords_cbe.head(10))\n",
    "\n",
    "# -------------------- GROUP KEYWORDS INTO THEMES --------------------\n",
    "# Manual theme mapping based on observed keywords\n",
    "theme_groups = {\n",
    "    \"Login Issues\": [\"login\", \"access\", \"account\", \"password\"],\n",
    "    \"Slow Performance\": [\"slow\", \"transfer\", \"load\", \"delay\"],\n",
    "    \"Good UX\": [\"ui\", \"interface\", \"design\", \"navigation\"],\n",
    "    \"Support Complaints\": [\"support\", \"help\", \"response\", \"contact\"],\n",
    "    \"Feature Requests\": [\"feature\", \"add\", \"fingerprint\", \"notification\"]\n",
    "}\n",
    "\n",
    "# -------------------- KEYWORD EXTRACTION PER BANK (Safe Version) --------------------\n",
    "def extract_keywords(text_series, max_features=30):\n",
    "    if text_series.isnull().all() or text_series.str.strip().str.len().sum() == 0:\n",
    "        raise ValueError(\"No valid text to process.\")\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features, ngram_range=(1, 2))\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_series)\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.sum(axis=0).A1\n",
    "    keywords_df = pd.DataFrame({'keyword': features, 'score': scores})\n",
    "    return keywords_df.sort_values(by='score', ascending=False)\n",
    "\n",
    "# -------------------- PROCESS ALL BANKS --------------------\n",
    "print(\"\\nTop Keywords by Bank:\")\n",
    "for bank_name in df['bank'].dropna().unique():\n",
    "    subset = df[df['bank'] == bank_name]\n",
    "    \n",
    "    # Filter out too-short reviews\n",
    "    valid_reviews = subset['cleaned_review'].dropna()\n",
    "    valid_reviews = valid_reviews[valid_reviews.str.strip().str.len() > 10]\n",
    "\n",
    "    if len(valid_reviews) == 0:\n",
    "        print(f\"\\n⚠️ Skipped '{bank_name}' — no valid reviews to process.\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        keywords = extract_keywords(valid_reviews)\n",
    "        print(f\"\\n🔹 {bank_name}:\\n\", keywords.head(10))\n",
    "    except ValueError:\n",
    "        print(f\"\\n⚠️ Skipped '{bank_name}' — empty vocabulary after cleaning.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
